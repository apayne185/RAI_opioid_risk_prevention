{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b54679",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This notebook is part of a machine learning for healthcare exercise, focusing on using the Responsible AI (RAI) package to enhance clinical decision-making. The toolkit will be used to analyze opioid use disorder (OD) risk, with three key objectives:\n",
    "\n",
    "1. Analyze Errors and Explore Interpretability of Models: We will run Interpret-Community’s global explainers to generate feature importance insights and visualize model errors with the Error Analysis dashboard\n",
    "\n",
    "2. Plan real-world action through counterfactual and causal analysis: By leveraging counterfactual examples and causal inference, we will explore decision-making strategies based on opioid prescription patterns and patient comorbidities to understand possible interventions and their impacts\n",
    "\n",
    "3. Assess addiction risk predictions: A classification model trained on patient-level features (income, surgeries, opioid prescription days, and comorbidities A–V) will be evaluated to examine its performance in predicting risk of opioid use disorder and to inform prevention strategies\n",
    "\n",
    "**The goal is to provide non-trivial insights for clinical decision making, leveraging machine learning paired with responsible AI tools, to improve patient outcomes in the healthcare context.**\n",
    "\n",
    "Based on notebooks from the [Responsible AI toolkit](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934c151",
   "metadata": {},
   "source": [
    "# 2. Data Set Characteristics\n",
    "\n",
    "Number of Instances: patient-level records (rows)\n",
    "\n",
    "Number of Attributes: 20 predictive attributes and 1 target class\n",
    "\n",
    "Attribute Information:\n",
    "- OD (target): whether the patient had an opioid use disorder diagnosis (binary: 1 = yes, 0 = no)\n",
    "- Low_inc: low income flag (1 = low income, 0 = not low income)\n",
    "- Surgery: whether the patient underwent major surgery in the 2 years\n",
    "- rx_ds: number of days of prescribed opioids in the 2 years\n",
    "- A: infectious diseases group A (binary flag)\n",
    "- B: infectious diseases group B\n",
    "- C: malignant neoplasm\n",
    "- D: benign neoplasm\n",
    "- E: endocrine conditions\n",
    "- F: mental and behavioral health conditions (excluding opioid-related)\n",
    "- H: ear conditions\n",
    "- I: circulatory system conditions\n",
    "- J: respiratory system conditions\n",
    "- K: digestive system conditions\n",
    "- L: skin conditions\n",
    "- M: musculoskeletal system conditions\n",
    "- N: urinary system conditions\n",
    "- R: other signs and symptoms\n",
    "- S: injuries\n",
    "- T: burns and toxic conditions\n",
    "- V: external trauma conditions\n",
    "\n",
    "class:\n",
    "- OD = 1: patient identified with opioid use disorder in the 2 years\n",
    "- OD = 0: patient without opioid use disorder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da36d3",
   "metadata": {},
   "source": [
    "# 3. Setup\n",
    "- responsibleai and raiwidgets provide RAIInsights and the dashboard\n",
    "- fairlearn provides fairness metrics and mitigation algorithms used under the hood\n",
    "- imbalanced-learn offers resampling utilities if you want to experiment with imbalance mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,           # Measures the ability of the model to distinguish between classes (higher is better)\n",
    "    average_precision_score, # Computes the average precision for precision-recall curves (useful for imbalanced data)\n",
    "    brier_score_loss,        # Measures the mean squared difference between predicted probabilities and actual outcomes (lower is better)\n",
    "    log_loss,                # Penalizes false classifications with a focus on probability estimates (lower is better)\n",
    "    confusion_matrix,        # Summarizes true/false positives/negatives for classification predictions\n",
    "    precision_score,         # Proportion of positive identifications that were actually correct (TP / (TP + FP))\n",
    "    recall_score,            # Proportion of actual positives that were correctly identified (TP / (TP + FN))\n",
    "    RocCurveDisplay,         \n",
    "    PrecisionRecallDisplay,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# Fairness utilities\n",
    "try:\n",
    "    from fairlearn.metrics import (\n",
    "        MetricFrame,\n",
    "        selection_rate,\n",
    "        true_positive_rate,\n",
    "        false_positive_rate,\n",
    "        false_negative_rate,\n",
    "    )\n",
    "    _FAIRLEARN = True\n",
    "except Exception as exc:\n",
    "    _FAIRLEARN = False\n",
    "    print(f\"Fairlearn metrics unavailable: {exc}\")\n",
    "\n",
    "# Responsible AI dashboard (optional)\n",
    "try:\n",
    "    from responsibleai import RAIInsights\n",
    "    from responsibleai.feature_metadata import FeatureMetadata\n",
    "    from raiwidgets import ResponsibleAIDashboard, ExplanationDashboard\n",
    "    from interpret_community.tabular_explainer import TabularExplainer\n",
    "    from erroranalysis import ModelAnalyzer\n",
    "    _RAI = True\n",
    "except Exception as exc:\n",
    "    _RAI = False\n",
    "    print(f\"Responsible AI extras unavailable: {exc}\")\n",
    "\n",
    "try:\n",
    "    import dice_ml  # optional counterfactual support\n",
    "    _DICE = True\n",
    "except Exception:\n",
    "    _DICE = False\n",
    "    print(\"Counterfactual component disabled: install dice-ml.\")\n",
    "\n",
    "try:\n",
    "    import econml  # optional causal analysis support\n",
    "    _ECONML = True\n",
    "except Exception:\n",
    "    _ECONML = False\n",
    "    print(\"Causal component disabled: install econml.\")\n",
    "\n",
    "# Course utilities for transparency and thresholding\n",
    "from utils import (\n",
    "    positive_scores,\n",
    "    auc_report,\n",
    "    tradeoff_table,\n",
    "    pick_threshold_cost,\n",
    "    pick_threshold_recall_floor,\n",
    "    pick_threshold_workload,\n",
    "    summary_at_threshold,\n",
    "    plot_recall_floor_curves,\n",
    "    plot_cumulative_recall_at_threshold,\n",
    "    plot_topk_at_threshold,\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc05cf",
   "metadata": {},
   "source": [
    "# 4. Data Load & preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f881f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your CSV (update if needed for your system)\n",
    "DATA_PATH = \"./Data/opiod_raw_data.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome prevalence and missingness\n",
    "\n",
    "TARGET = \"OD\"  # target: 1 = opioid use disorder diagnosis, 0 = none\n",
    "\n",
    "# Outcome prevalence\n",
    "counts = df_raw[TARGET].value_counts(dropna=False)\n",
    "prevalence_percent = counts[1] / counts.sum() * 100\n",
    "positives_per_1000 = counts[1] / counts.sum() * 1000\n",
    "\n",
    "print(\"Outcome counts:\")\n",
    "print(counts)\n",
    "print(f\"\\nPrevalence: {prevalence_percent:.2f}%\")\n",
    "print(f\"Patients with OD per 1000: {positives_per_1000:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142570c9",
   "metadata": {},
   "source": [
    "## 4.1. Basic cleaning and schema alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9316e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the raw DataFrame\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = [c.strip().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "              for c in df.columns]\n",
    "\n",
    "# Drop ID column\n",
    "if df.shape[1] > 0:\n",
    "    df = df.drop(columns=[df.columns[0]])\n",
    "\n",
    "# Harmonize known aliases\n",
    "if \"SURG\" in df.columns and \"Surgery\" not in df.columns:\n",
    "    df = df.rename(columns={\"SURG\": \"Surgery\"})\n",
    "\n",
    "# Expected columns from the data dictionary\n",
    "expected_cols = [\n",
    "    \"OD\", \"Low_inc\", \"Surgery\", \"rx_ds\",\n",
    "    \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"R\", \"S\", \"T\", \"V\"\n",
    "]\n",
    "\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "# Helper to coerce various binary encodings to 0/1\n",
    "\n",
    "\n",
    "def to_binary(s: pd.Series) -> pd.Series:\n",
    "    if s.dtype == \"O\":\n",
    "        mapped = s.astype(str).str.strip().str.lower().map({\n",
    "            \"1\": 1, \"0\": 0,\n",
    "            \"y\": 1, \"n\": 0,\n",
    "            \"yes\": 1, \"no\": 0,\n",
    "            \"true\": 1, \"false\": 0\n",
    "        })\n",
    "        s = pd.to_numeric(mapped, errors=\"coerce\")\n",
    "    else:\n",
    "        s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return (s.fillna(0) > 0).astype(int)\n",
    "\n",
    "\n",
    "# Target is binary 0/1\n",
    "df[\"OD\"] = to_binary(df[\"OD\"])\n",
    "\n",
    "# rx_ds is numeric count of opioid prescription days\n",
    "df[\"rx_ds\"] = pd.to_numeric(df[\"rx_ds\"], errors=\"coerce\")\n",
    "\n",
    "# Binary predictors: Low_inc, Surgery, and A..V\n",
    "binary_cols = [\"Low_inc\", \"Surgery\", \"A\", \"B\", \"C\", \"D\", \"E\",\n",
    "               \"F\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"R\", \"S\", \"T\", \"V\"]\n",
    "df[binary_cols] = df[binary_cols].apply(to_binary)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34345072",
   "metadata": {},
   "source": [
    "# 5. Training, Validation and Testing - 80/15/5 stratified split\n",
    "\n",
    "Train–test split\n",
    "\n",
    "- Prevents “peeking” at data and overestimating performance\n",
    "- Mimics real-world deployment where models face unseen patients\n",
    "- Always evaluate on data not used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"OD\"\n",
    "num_cols = [\"rx_ds\"]\n",
    "cat_like_binary_cols = binary_cols.copy()\n",
    "\n",
    "X = df[num_cols + cat_like_binary_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Split temp into validation and test, 15% each overall\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE\n",
    ")\n",
    "# Display shapes and class distributions as a DataFrame\n",
    "\n",
    "info = {\n",
    "    \"Set\": [\"Train\", \"Validation\", \"Test\", \"Overall\"],\n",
    "    \"X shape\": [X_train.shape, X_val.shape, X_test.shape, X.shape],\n",
    "    \"y shape\": [y_train.shape, y_val.shape, y_test.shape, y.shape],\n",
    "    \"p(OD=0)\": [\n",
    "        y_train.value_counts(normalize=True).get(0, 0.0),\n",
    "        y_val.value_counts(normalize=True).get(0, 0.0),\n",
    "        y_test.value_counts(normalize=True).get(0, 0.0),\n",
    "        y.value_counts(normalize=True).get(0, 0.0),\n",
    "    ],\n",
    "    \"p(OD=1)\": [\n",
    "        y_train.value_counts(normalize=True).get(1, 0.0),\n",
    "        y_val.value_counts(normalize=True).get(1, 0.0),\n",
    "        y_test.value_counts(normalize=True).get(1, 0.0),\n",
    "        y.value_counts(normalize=True).get(1, 0.0),\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_info = pd.DataFrame(info)\n",
    "display(df_info.style.format({\"p(OD=0)\": \"{:.3f}\", \"p(OD=1)\": \"{:.3f}\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0a7ff",
   "metadata": {},
   "source": [
    "## 5.1. Modeling pipeline, training, and calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63d7b7",
   "metadata": {},
   "source": [
    "### Setting a baseline\n",
    "A naive majority-class baseline clarifies the minimum standard any model must beat, highlighting the danger of ignoring minority patients and ensuring improvements carry meaningful weight in healthcare decision making\n",
    "\n",
    "**ROC AUC**  \n",
    "- 0.5 → no discrimination\n",
    "- 0.6–0.7 → poor\n",
    "- 0.7–0.8 → fair\n",
    "- 0.8–0.9 → good\n",
    "- ≥ 0.9 → excellent\n",
    "\n",
    "**PR AUC**  \n",
    "- Must be interpreted against event prevalence `p` in the validation set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: DummyClassifier (majority class)\n",
    "dummy_clf = DummyClassifier(\n",
    "    strategy=\"most_frequent\", random_state=RANDOM_STATE)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "proba_val = dummy_clf.predict_proba(X_val)\n",
    "pos_idx = int(np.where(dummy_clf.classes_ == 1)[0][0])  # index for class \"1\"\n",
    "y_score_val = np.asarray(proba_val)[:, pos_idx]\n",
    "\n",
    "# Evaluate on validation\n",
    "y_score_val = positive_scores(dummy_clf, X_val)\n",
    "metrics_dummy = auc_report(\n",
    "    y_val, y_score_val, name=\"Dummy baseline\", plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bc2a2",
   "metadata": {},
   "source": [
    "## 5.2. Preprocesing our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74d963",
   "metadata": {},
   "source": [
    "[Scikit-learn preprocessing](https://scikit-learn.org/stable/api/sklearn.preprocessing.html) standardizes and transforms features for modeling, including scaling, encoding, and imputation. Helping to maintain a consistent data transformation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True))\n",
    "])\n",
    "\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"bin\", binary_transformer, cat_like_binary_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",  # drops any column not in previously specified\n",
    "    verbose_feature_names_out=False  # keeps original feature names\n",
    ")\n",
    "\n",
    "# Logistic Regression baseline with variance filter\n",
    "base_clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    # Eliminates features with zero variance\n",
    "    (\"varth\", VarianceThreshold(threshold=0.0)),\n",
    "    (\"model\", LogisticRegression(\n",
    "        solver=\"liblinear\",  # See details in course material\n",
    "        class_weight=\"balanced\",  # Adjusts weights for class imbalance\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=200\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f8ccc7",
   "metadata": {},
   "source": [
    "### Basic preliminary model performance check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit base model\n",
    "base_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation\n",
    "y_score_val = positive_scores(base_clf, X_val)\n",
    "metrics_base = auc_report(y_val, y_score_val, name=\"base_clf\", plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4634f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five patients: raw pre-calibration score, predicted label, actual label\n",
    "\n",
    "tbl5 = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"raw_score\": positive_scores(base_clf, X_val),\n",
    "            \"actual\": y_val.loc[X_val.index].astype(int),\n",
    "        },\n",
    "        index=X_val.index,\n",
    "    )\n",
    "    .assign(predicted=lambda d: (d[\"raw_score\"] >= 0.50).astype(int))  # threshold before calibration\n",
    "    .round({\"raw_score\": 3})\n",
    "    .sample(n=10, random_state=7)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"patient\"})\n",
    "    .loc[:, [\"patient\", \"raw_score\", \"predicted\", \"actual\"]]\n",
    ")\n",
    "\n",
    "display(tbl5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the plot using variables from the preceding cells\n",
    "scores = positive_scores(base_clf, X_val)\n",
    "actuals = y_val\n",
    "\n",
    "# Separate scores for actual positive (OD=1) and negative (OD=0) cases\n",
    "scores_positive = scores[actuals == 1]\n",
    "scores_negative = scores[actuals == 0]\n",
    "\n",
    "# Generate random jitter for the x-axis\n",
    "jitter_strength = 0.03\n",
    "jitter_positive = np.random.normal(0, jitter_strength, len(scores_positive))\n",
    "jitter_negative = np.random.normal(0, jitter_strength, len(scores_negative))\n",
    "\n",
    "# 3. Create the plot\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "scatter_other = ax.scatter(jitter_negative, scores_negative, color='darkslategrey', alpha=0.3, label='Other')\n",
    "scatter_target = ax.scatter(jitter_positive, scores_positive, color='red', alpha=0.7, label='OD')\n",
    "\n",
    "# Add threshold lines\n",
    "line_50 = ax.axhline(y=0.5, color='red', linestyle='-', linewidth=2, label='Threshold = 0.5')\n",
    "line_80 = ax.axhline(y=0.7, color='orange', linestyle='--', linewidth=2, label='Threshold = 0.7')\n",
    "\n",
    "ax.set_title('Scores vs. Actuals')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xticks([])\n",
    "ax.set_xlim(-0.2, 0.2) # Set fixed x-axis limits to control the visual spread\n",
    "ax.legend(handles=[scatter_target, scatter_other, line_50, line_80], fontsize=9, loc='upper right')\n",
    "\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e413ac1",
   "metadata": {},
   "source": [
    "# 6. Recalibrating the Scores\n",
    "\n",
    "**1. Reliable probabilities**  \n",
    "- Turns raw scores into real probabilities  \n",
    "- Ensures predictions match observed outcome frequencies  \n",
    "- Prevents overly high or low risk estimates  \n",
    "\n",
    "**2. Better clinical decisions**  \n",
    "- Essential when risk values guide medical choices  \n",
    "- Supports thresholds with clear clinical meaning\n",
    "- Reduces wasted clinical resources  \n",
    "\n",
    "**3. Trust and adoption**  \n",
    "- Builds trust in AI decisions  \n",
    "- Enables safer patient outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate probabilities using CV on training data\n",
    "calibrated_clf = CalibratedClassifierCV(\n",
    "    estimator=base_clf,\n",
    "    method=\"sigmoid\",\n",
    "    cv=5\n",
    ")\n",
    "calibrated_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile bands on test only\n",
    "base_clf_uncal = clone(base_clf).fit(X_train, y_train)\n",
    "p_before = base_clf_uncal.predict_proba(X_test)[:, 1]\n",
    "p_after = calibrated_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "df_cal = pd.DataFrame({\"Actual_OD\": y_test.values, \"Pred_before\": p_before,\n",
    "                      \"Pred_after\": p_after}, index=X_test.index)\n",
    "\n",
    "# 10 quantile bands with similar counts\n",
    "df_cal[\"Risk_band\"] = pd.qcut(df_cal[\"Pred_after\"], q=10, labels=[\n",
    "                              f\"Q{i}\" for i in range(1, 11)], duplicates=\"drop\")\n",
    "\n",
    "summary = (\n",
    "    df_cal.groupby(\"Risk_band\", observed=True)\n",
    "    .agg(Patients=(\"Actual_OD\", \"size\"),\n",
    "         Avg_pred_before=(\"Pred_before\", \"mean\"),\n",
    "         Actual_OD_rate=(\"Actual_OD\", \"mean\"),\n",
    "         Avg_pred_after=(\"Pred_after\", \"mean\"))\n",
    "    .reset_index()\n",
    "    .round(3)\n",
    ")\n",
    "display(summary)\n",
    "\n",
    "print(\"Check sizes\")\n",
    "print(\"len(X_train) =\", len(X_train), \"len(X_val) =\",\n",
    "      len(X_val), \"len(X_test) =\", len(X_test))\n",
    "print(\"Rows in table sum to\", int(summary[\"Patients\"].sum()))\n",
    "\n",
    "# Reliability plot using the same fixed risk bands summary\n",
    "print(\"Points near the diagonal mean predicted risk matches observed OD frequency\")\n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\",\n",
    "         linewidth=1, label=\"Perfectly calibrated\")\n",
    "plt.scatter(summary[\"Avg_pred_before\"],\n",
    "            summary[\"Actual_OD_rate\"], label=\"Before calibration\")\n",
    "plt.scatter(summary[\"Avg_pred_after\"],\n",
    "            summary[\"Actual_OD_rate\"],  label=\"After calibration\")\n",
    "plt.xlabel(\"Predicted risk\")\n",
    "plt.ylabel(\"Observed OD rate\")\n",
    "plt.title(\"Calibration reliability by risk bands\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234181a1",
   "metadata": {},
   "source": [
    "### Compare base_clf vs calibrated_clf on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d490f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on discrimination equality and calibration improvements\n",
    "\n",
    "# Scores\n",
    "y_score_val_base = positive_scores(base_clf, X_val)\n",
    "y_score_val_cal = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Discrimination\n",
    "roc_base = roc_auc_score(y_val, y_score_val_base)\n",
    "pr_base = average_precision_score(y_val, y_score_val_base)\n",
    "roc_cal = roc_auc_score(y_val, y_score_val_cal)\n",
    "pr_cal = average_precision_score(y_val, y_score_val_cal)\n",
    "\n",
    "# Calibration\n",
    "ll_base = log_loss(y_val, np.clip(y_score_val_base, 1e-6, 1 - 1e-6))\n",
    "ll_cal = log_loss(y_val, np.clip(y_score_val_cal,  1e-6, 1 - 1e-6))\n",
    "brier_base = brier_score_loss(y_val, y_score_val_base)\n",
    "brier_cal = brier_score_loss(y_val, y_score_val_cal)\n",
    "\n",
    "# Assemble into dataframe\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\"Metric\": \"ROC AUC\", \"Base\": roc_base, \"Calibrated\": roc_cal, \"Explanation\": \"Ability to distinguish between classes (higher is better)\"},\n",
    "    {\"Metric\": \"PR AUC\", \"Base\": pr_base, \"Calibrated\": pr_cal, \"Explanation\": \"Precision-recall curve area; useful for imbalanced data\"},\n",
    "    {\"Metric\": \"Log loss\", \"Base\": ll_base, \"Calibrated\": ll_cal, \"Explanation\": \"Penalty for incorrect and overconfident predictions (lower is better)\"},\n",
    "    {\"Metric\": \"Brier score\", \"Base\": brier_base, \"Calibrated\": brier_cal, \"Explanation\": \"Mean squared error of predicted probabilities (lower is better)\"},\n",
    "])\n",
    "\n",
    "display(metrics_df.style.format({\"Base\": \"{:.3f}\", \"Calibrated\": \"{:.3f}\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c73af",
   "metadata": {},
   "source": [
    "# 7. Deciding where to cut off i.e. what probability is “high risk enough” to trigger an intervention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96638f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get calibrated probabilities\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Define a grid of thresholds\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "rows = []\n",
    "for thr in thresholds:\n",
    "    y_pred = (y_score_val >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    precision = precision_score(y_val, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred, zero_division=0)\n",
    "    alerts_per_1000 = 1000 * np.mean(y_pred)\n",
    "    true_pos_per_1000 = 1000 * tp / len(y_val)\n",
    "    rows.append({\n",
    "        \"threshold\": thr,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"alerts_per_1000\": alerts_per_1000,\n",
    "        \"true_pos_per_1000\": true_pos_per_1000,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn\n",
    "    })\n",
    "\n",
    "df_thr = pd.DataFrame(rows)\n",
    "\n",
    "# Display a few candidate thresholds for discussion\n",
    "display(\n",
    "    df_thr.query(\"threshold in [0.1, 0.2, 0.3, 0.4, 0.5]\")\n",
    "         .round(3)\n",
    "         .set_index(\"threshold\")\n",
    ")\n",
    "\n",
    "# Plot workload vs threshold\n",
    "plt.figure()\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"alerts_per_1000\"], label=\"Alerts per 1000\")\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"true_pos_per_1000\"], label=\"True positives per 1000\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Count per 1000 patients\")\n",
    "plt.title(\"Operational tradeoffs vs threshold (validation set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cd2f5",
   "metadata": {},
   "source": [
    "## 7.1. Choosing an operating threshold\n",
    "\n",
    "Models produce probabilities, but clinicians are the ones taking decisions, and carring the accountability of their actions.\n",
    "\n",
    "- Setting a threshold balances in this case, among others, between missed addiction cases and unnecessary undertreatment of pain\n",
    "- Clear rules make these tradeoffs explicit, explainable, and auditable!\n",
    "\n",
    "We will run three threshold tuning analyses:\n",
    "1. **Workload constrained threshold**  \n",
    "  Capture the most true cases without exceeding a fixed alert capacity for the clinic\n",
    "2. **Recall floor then maximize precision**  \n",
    "  Guarantee a minimum case capture for safety, then pick the threshold with the fewest false alarms\n",
    "3. **Cost based threshold (Bayes rule)**  \n",
    "  Minimize expected harm using estimated costs for false negatives and false positives\n",
    "\n",
    "Readouts to watch\n",
    "- Threshold, precision, recall, alerts per 1000 patients, true positives per 1000, false positives, false negatives\n",
    "- Connect the chosen rule to clinical policy and resource capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f08220",
   "metadata": {},
   "source": [
    "### 7.1.1. Workload constrained threshold for calibrated_clf on validation\n",
    "\n",
    "- Alerts budget: maximum alerts per 1000 patients the clinic can review without overloading resources\n",
    "- Objective: within the alerts budget, choose the threshold that yields the most true positives per 1000 so more patients at real risk are correctly flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ad3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALERTS_BUDGET = 100.0  # alerts per 1000 patients\n",
    "\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "res = pick_threshold_workload(y_val, y_score_val, alerts_per_1000_max=ALERTS_BUDGET)\n",
    "tbl = res[\"table\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"alerts_per_1000\"], label=\"Alerts per 1000\")\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"true_pos_per_1000\"], label=\"True positives per 1000\")\n",
    "plt.axhline(ALERTS_BUDGET, linestyle=\"--\")\n",
    "plt.axvline(res[\"threshold\"], linestyle=\":\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Count per 1000\")\n",
    "plt.title(\"Workload constrained threshold\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# make sure summary_df is a DataFrame\n",
    "summary_df = res[\"summary\"] if isinstance(res[\"summary\"], pd.DataFrame) else pd.DataFrame(res[\"summary\"])\n",
    "\n",
    "# put metrics in the rows, rules in the columns\n",
    "pivot_df = summary_df.set_index(\"rule\").T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    "display(pivot_df.round(3).set_index(\"metric\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce0f3a8",
   "metadata": {},
   "source": [
    "Let's now see the risk groups by decile (bands), and visualizes the overall risk score distribution, with optional threshold overlay and decile boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ac85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BINS = 10  # deciles by default\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Build bands by quantiles, highest risk = band 1\n",
    "bands = pd.qcut(y_score_val, q=N_BINS, labels=False, duplicates=\"drop\")\n",
    "# qcut labels lowest=0..highest=K-1, invert so 1 is highest-risk band\n",
    "bands = (bands.max() - bands) + 1\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"y_true\": y_val.astype(int),\n",
    "    \"score\": y_score_val,\n",
    "    \"band\": bands.astype(int),\n",
    "})\n",
    "\n",
    "summ = (df.groupby(\"band\", as_index=True)\n",
    "          .agg(n=(\"y_true\",\"size\"),\n",
    "               positives=(\"y_true\",\"sum\"),\n",
    "               min_score=(\"score\",\"min\"),\n",
    "               max_score=(\"score\",\"max\"))\n",
    "          .sort_index())\n",
    "\n",
    "summ[\"prevalence\"] = summ[\"positives\"] / summ[\"n\"]\n",
    "summ[\"cum_capture\"] = summ[\"positives\"].cumsum() / df[\"y_true\"].sum()\n",
    "summ[\"alerts_per_1000\"] = 1000.0 * summ[\"n\"] / len(df)\n",
    "summ[\"true_pos_per_1000\"] = 1000.0 * summ[\"positives\"] / len(df)\n",
    "\n",
    "display(summ.round(3))\n",
    "\n",
    "# Optional threshold overlay if you already chose one, else set THR=None\n",
    "THR = None  # e.g., THR = 0.23\n",
    "\n",
    "# Histogram of risk scores with decile edges\n",
    "plt.figure()\n",
    "plt.hist(df[\"score\"], bins=30)\n",
    "if THR is not None:\n",
    "    plt.axvline(THR, linestyle=\"--\")\n",
    "# draw decile boundaries\n",
    "edges = np.quantile(df[\"score\"], np.linspace(0,1,N_BINS+1))\n",
    "for x in edges:\n",
    "    plt.axvline(x, linestyle=\":\", linewidth=0.8)\n",
    "plt.xlabel(\"Predicted risk\")\n",
    "plt.ylabel(\"Patients\")\n",
    "plt.title(\"Risk distribution with decile boundaries\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf7d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "THR = res[\"threshold\"] # <- use the threshold from the previous step\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "ids = np.arange(len(y_val))\n",
    "\n",
    "# Sort patients by predicted risk\n",
    "order = np.argsort(-y_score_val)\n",
    "top_idx = order[:30]   # top 30 for visualization\n",
    "top_scores = y_score_val[top_idx]\n",
    "top_true = np.asarray(y_val)[top_idx].astype(int)\n",
    "\n",
    "# Split indices for TP vs FP\n",
    "tp_idx = np.where(top_true == 1)[0]\n",
    "fp_idx = np.where(top_true == 0)[0]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(tp_idx, top_scores[tp_idx], color=\"tab:red\", label=\"True addicted (TP)\")\n",
    "plt.bar(fp_idx, top_scores[fp_idx], color=\"tab:gray\", label=\"Not addicted (FP)\")\n",
    "plt.axhline(THR, linestyle=\"--\", color=\"black\", label=f\"Threshold = {THR:.2f}\")\n",
    "\n",
    "plt.xlabel(\"Patients ranked by predicted risk\")\n",
    "plt.ylabel(\"Predicted risk\")\n",
    "plt.title(\"Top 30 highest-risk patients on validation\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195459c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "order = np.argsort(-y_score_val)\n",
    "y_sorted = np.asarray(y_val)[order].astype(int)\n",
    "\n",
    "# Cumulative recall\n",
    "cum_tp = np.cumsum(y_sorted)\n",
    "total_pos = cum_tp[-1] if cum_tp.size else 0\n",
    "alerts = np.arange(1, len(y_sorted) + 1)\n",
    "recall_curve = cum_tp / total_pos if total_pos > 0 else np.zeros_like(cum_tp)\n",
    "\n",
    "# Budget for alerts\n",
    "n_budget = int(np.ceil(ALERTS_BUDGET * len(y_val) / 1000.0))\n",
    "\n",
    "# Recall at budget\n",
    "recall_at_budget = recall_curve[n_budget - 1] if n_budget > 0 and n_budget <= len(y_val) else 0.0\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(alerts, recall_curve, label=\"Cumulative recall\")\n",
    "plt.axvline(n_budget, linestyle=\"--\", color=\"red\", label=f\"Budget = {n_budget} alerts\")\n",
    "\n",
    "# Annotate recall at budget\n",
    "plt.scatter(n_budget, recall_at_budget, color=\"black\", zorder=5)\n",
    "plt.text(n_budget + 2, recall_at_budget, f\"Recall = {recall_at_budget:.2f}\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"Number of alerts\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Cumulative capture of true cases vs alerts\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e54706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative precision at top-k alerts\n",
    "alerts = np.arange(1, len(y_sorted) + 1)\n",
    "cum_tp = np.cumsum(y_sorted)\n",
    "precision_curve = cum_tp / alerts\n",
    "\n",
    "# Alerts budget scaled to validation size\n",
    "prec_at_budget = precision_curve[n_budget - 1] if 0 < n_budget <= len(y_sorted) else 0.0\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(alerts, precision_curve, label=\"Precision at top-k alerts\")\n",
    "plt.axvline(n_budget, linestyle=\"--\", label=f\"Budget = {n_budget} alerts\")\n",
    "plt.scatter(n_budget, prec_at_budget, zorder=5)\n",
    "plt.text(n_budget + max(2, len(y_sorted)//100), prec_at_budget, f\"Precision = {prec_at_budget:.2f}\", va=\"center\")\n",
    "\n",
    "plt.xlabel(\"Number of alerts\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs number of alerts\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c8fbb",
   "metadata": {},
   "source": [
    "### 7.1.2. Recall floor then maximize precision for calibrated_clf on validation\n",
    "\n",
    "- Recall floor: minimum acceptable recall set by safety policy to limit missed addiction cases\n",
    "- Precision objective: among thresholds meeting the recall floor, pick the one with highest precision to reduce unnecessary undertreatment and clinician workload\n",
    "\n",
    "Deciding which recall floor to sue:\n",
    "1. The chosen floor is a value judgment balancing patient safety vs resource burden\n",
    "2. In medicine, it's often the case that missing a true case (false negative) is often much worse than raising extra alarms (false positives)\n",
    "3. A recall floor enforces a safety guarantee: the model must capture at least e.g. 60% of patients who will become addicted\n",
    "\n",
    "Among thresholds that satisfy recall ≥ 0.6, you then pick the one with the best precision, to minimize unnecessary undertreatment and workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af300cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: choose threshold by recall floor\n",
    "RECALL_FLOOR = 0.60 # <- judgment call\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "res = pick_threshold_recall_floor(y_val, y_score_val, recall_floor=RECALL_FLOOR)\n",
    "THR = float(res[\"threshold\"])\n",
    "\n",
    "summary_df = summary_at_threshold(y_val, y_score_val, THR)\n",
    "pivot_df = summary_df.set_index(\"threshold\").T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    "display(pivot_df.round(3).set_index(\"metric\"))\n",
    "\n",
    "# Precision and recall vs threshold with annotations\n",
    "plot_recall_floor_curves(y_val, y_score_val, recall_floor=RECALL_FLOOR, chosen_threshold=THR)\n",
    "\n",
    "# Cumulative recall vs alerts with vertical line at alerts implied by THR\n",
    "plot_cumulative_recall_at_threshold(y_val, y_score_val, chosen_threshold=THR)\n",
    "\n",
    "# Patient-level prioritization view at THR\n",
    "plot_topk_at_threshold(y_val, y_score_val, chosen_threshold=THR, top_k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48885c",
   "metadata": {},
   "source": [
    "### 7.1.3. Cost based threshold for calibrated_clf on validation\n",
    "\n",
    "This approach selects a threshold that minimizes the expected cost of errors. We need to estimate the relative costs of false negatives (Givin opioid drugs to people who would actually get addited) and false positives (Depriving low risk people from receiving the medicine, costs in other terapies, etc.).\n",
    "\n",
    "- Cost of a False Negative (C_FN): The cost of missing a patient who will develop an opioid use disorder (e.g., costs of future intensive treatment, overdose events, and negative health outcomes, social impact, etc.)\n",
    "\n",
    "- Cost of a False Positive (C_FP): The cost of incorrectly flagging a patient as high-risk (e.g., costs of unnecessary clinical review, potential undertreatment of legitimate pain, and patient anxiety)\n",
    "\n",
    "For this exercise, we will assume a cost ratio where a FN is 10 times more costly than a false positive. The optimal threshold is then found by minimizing the total expected cost (C_FP * FP + C_FN * FN) on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set domain costs\n",
    "C_FN = 100.0  # Cost of a missed addiction case (False Negative)\n",
    "C_FP = 10.0   # Cost of undertreated pain (False Positive)\n",
    "\n",
    "# Get scores from the calibrated model on the validation set\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "\n",
    "# Find the optimal thresholds based on cost\n",
    "res = pick_threshold_cost(y_val, y_score_val, C_FP=C_FP, C_FN=C_FN)\n",
    "tbl = res[\"table\"].copy()\n",
    "tbl[\"expected_cost\"] = C_FP * tbl[\"FP\"] + C_FN * tbl[\"FN\"]\n",
    "\n",
    "# Display a summary table comparing the two threshold options\n",
    "summary_df = res[\"summary\"] if isinstance(res[\"summary\"], pd.DataFrame) else pd.DataFrame(res[\"summary\"])\n",
    "\n",
    "pivot_df = (\n",
    "    summary_df.set_index(\"rule\").T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    ")\n",
    "\n",
    "display(pivot_df.round(3).set_index(\"metric\"))\n",
    "\n",
    "# Plot expected cost vs. threshold\n",
    "plt.figure()\n",
    "plt.plot(tbl[\"threshold\"], tbl[\"expected_cost\"])\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Expected Cost\")\n",
    "plt.title(\"Cost-Based Threshold Selection (Validation Set)\")\n",
    "\n",
    "# Annotate the two optimal thresholds found\n",
    "t_formula = res[\"threshold_formula\"]\n",
    "t_emp = res[\"threshold_empirical\"]\n",
    "plt.axvline(t_formula, linestyle=\"--\", color=\"red\", label=f\"Bayes Optimal Thr={t_formula:.2f}\")\n",
    "plt.axvline(t_emp, linestyle=\":\", color=\"black\", label=f\"Empirical Min Thr={t_emp:.2f}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81633a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the empirically optimal threshold from the previous analysis\n",
    "THR = res[\"threshold_empirical\"]\n",
    "\n",
    "# For visualization, we need a \"recall_floor\" to draw the horizontal line.\n",
    "# We'll use the actual recall achieved at our chosen cost-based threshold.\n",
    "y_pred_val = (y_score_val >= THR).astype(int)\n",
    "recall_at_thr = recall_score(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Visualizing performance at the empirical cost-based threshold of {THR:.3f}:\")\n",
    "plot_recall_floor_curves(y_val, y_score_val, recall_floor=recall_at_thr, chosen_threshold=THR)\n",
    "plot_cumulative_recall_at_threshold(y_val, y_score_val, chosen_threshold=THR)\n",
    "plot_topk_at_threshold(y_val, y_score_val, chosen_threshold=THR, top_k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e712ce",
   "metadata": {},
   "source": [
    "# 8. Final Threshold Selection and Test Set Evaluation\n",
    "\n",
    "This is a crucial step that often involves discussion with clinical stakeholders to align the model's operating point with clinical goals, patient safety requirements, and resource availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8947a6",
   "metadata": {},
   "source": [
    "| Thresholding Method | Objective | Pros (Best for...) | Cons (Potential Risks) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **1. Workload Constrained** | Maximize TP (addiction risk) under a fixed (alert) budget | Good for limited resources, ensuring Ops stability | A low budget will miss many high-risk patients |\n",
    "| **2. Recall Floor** | Maximize precision (TP vs. FP) while holding a minimum recall rate (breath) | Acts as a safety net, ensuring a minimum capture rate | Increases false alarms e.g. workloads from alternative treatments, untreated pain |\n",
    "| **3. Cost-Based** | Minimize total expected cost by assigning costs to errors | Provides a formal f/ work to balance different clinical errors | Highly dependent on accurate cost estimates (can be subjective) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be2149",
   "metadata": {},
   "source": [
    "1. Compute the candidate threshold (THR) for each policy on the validation set\n",
    "2. Compare them on consistent operational metrics and expected cost\n",
    "3. Lock the final threshold using the same (THR) variable, to get an unbiased estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da659da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds are derived on validation, comparison is on test\n",
    "\n",
    "# Scores\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "y_score_test = positive_scores(calibrated_clf, X_test)\n",
    "\n",
    "# Derive thresholds on validation only, for the three methods\n",
    "res_rec  = pick_threshold_recall_floor(y_val, y_score_val, recall_floor=RECALL_FLOOR)\n",
    "thr_rec  = float(res_rec[\"threshold\"])\n",
    "\n",
    "res_work = pick_threshold_workload(y_val, y_score_val, alerts_per_1000_max=ALERTS_BUDGET)\n",
    "thr_work = float(res_work[\"threshold\"])\n",
    "\n",
    "res_cost = pick_threshold_cost(y_val, y_score_val, C_FP=C_FP, C_FN=C_FN)\n",
    "thr_cost_emp = float(res_cost[\"threshold_empirical\"])\n",
    "\n",
    "# Compare policies on the unseen test set using the frozen (validation) thresholds\n",
    "rows = []\n",
    "for approach, thr in [\n",
    "    (\"1. Workload budget\", thr_work),\n",
    "    (\"2. Recall floor\",    thr_rec),\n",
    "    (\"3. Cost min emp\",    thr_cost_emp),\n",
    "]:\n",
    "    s = summary_at_threshold(y_test, y_score_test, thr).iloc[0].to_dict()\n",
    "    s.update({\n",
    "        \"approach\": approach,\n",
    "        \"expected_cost\": float(C_FP * s[\"FP\"] + C_FN * s[\"FN\"]),\n",
    "    })\n",
    "    rows.append(s)\n",
    "\n",
    "test_compare = (\n",
    "    pd.DataFrame(rows)[\n",
    "        [\"approach\", \"threshold\", \"precision\", \"recall\",\n",
    "         \"alerts_per_1000\", \"true_pos_per_1000\", \"TP\", \"FP\", \"FN\", \"expected_cost\"]\n",
    "    ]\n",
    "    .sort_values(\"approach\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "pivot_df = (\n",
    "    test_compare.set_index(\"approach\").T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    ")\n",
    "\n",
    "display(pivot_df.round(3).set_index(\"metric\"))\n",
    "\n",
    "# Also report threshold-independent curve metrics on test for context\n",
    "test_roc_auc = float(roc_auc_score(y_test, y_score_test))\n",
    "test_pr_auc  = float(average_precision_score(y_test, y_score_test))\n",
    "print(f\"Test ROC AUC={test_roc_auc:.3f}  |  Test PR AUC={test_pr_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648aedc",
   "metadata": {},
   "source": [
    "Based on this scenario:\n",
    "\n",
    "- **Workload Constraint (THR = 0.38)**: Offers the highest precision (0.667) but the lowest recall (0.222), missing 7 of the 9 actual OD cases. This might be operationally efficient but carries a high patient risk i.e. low coverage for all patients\n",
    "\n",
    "- **Recall Floor (THR = 0.28)**: Guarantees a minimum level of patient safety by capturing over half the cases (recall = 0.556), at the cost of more false positive alerts i.e. more patients without opioid treatment (alternative treatments $$$)\n",
    "\n",
    "- **Cost-Based (THR = 0.17)**: Provides the highest recall (0.889) and lowest \"expected cost\" based on our 10:1 cost ratio. However, it generates the most alerts (460 per 1000 patients), which may not be operationally feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65540e10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb6f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_compare = test_compare.sort_values('approach').reset_index(drop=True)\n",
    "\n",
    "# Extract stats for each approach from the DataFrame\n",
    "workload_stats = test_compare.iloc[0]\n",
    "recall_stats = test_compare.iloc[1]\n",
    "cost_stats = test_compare.iloc[2]\n",
    "\n",
    "# Calculate total positive cases in the test set\n",
    "total_pos_test = workload_stats['TP'] + workload_stats['FN']\n",
    "\n",
    "# Dynamic markdown with increased font size using HTML <div>\n",
    "markdown_text = f\"\"\"\n",
    "* **Workload Constraint (THR = {workload_stats['threshold']:.2f}):** Offers the highest precision ({workload_stats['precision']:.3f}) but the lowest recall ({workload_stats['recall']:.3f}), missing {workload_stats['FN']:.0f} of the {total_pos_test:.0f} actual OD cases. This might be operationally efficient but carries a high patient risk\n",
    "* **Recall Floor (THR = {recall_stats['threshold']:.2f}):** Guarantees a minimum level of patient safety by capturing over half the cases (recall = {recall_stats['recall']:.3f}), at the cost of more false positive alerts\n",
    "* **Cost-Based (THR = {cost_stats['threshold']:.2f}):** Provides the highest recall ({cost_stats['recall']:.3f}) and lowest \"expected cost\" based on our 10:1 cost ratio. However, it generates the most alerts ({cost_stats['alerts_per_1000']:.0f} per 1000 patients), which may not be operationally feasible\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88cd2c",
   "metadata": {},
   "source": [
    "**Potential Decision:** For this clinical use case, patient safety is paramount. Missing a potential opioid use disorder case (a False Negative) has a significantly higher societal and health cost than a false alarm (a False Positive). Therefore, we will adopt the **2. Recall Floor threshold** of 0.28.\n",
    "\n",
    "This choice ensures we identify a majority of at-risk patients (recall > 0.5) while maintaining a manageable number of alerts for clinical review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99676342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scores and validation-derived thresholds (from your original code) ---\n",
    "y_score_val = positive_scores(calibrated_clf, X_val)\n",
    "y_score_test = positive_scores(calibrated_clf, X_test)\n",
    "\n",
    "res_rec  = pick_threshold_recall_floor(y_val, y_score_val, recall_floor=RECALL_FLOOR)\n",
    "thr_rec  = float(res_rec[\"threshold\"])\n",
    "\n",
    "res_work = pick_threshold_workload(y_val, y_score_val, alerts_per_1000_max=ALERTS_BUDGET)\n",
    "thr_work = float(res_work[\"threshold\"])\n",
    "\n",
    "res_cost = pick_threshold_cost(y_val, y_score_val, C_FP=C_FP, C_FN=C_FN)\n",
    "thr_cost_emp = float(res_cost[\"threshold_empirical\"])\n",
    "\n",
    "# --- Find the F1-optimal threshold on the VALIDATION set ---\n",
    "thresholds_f1 = np.linspace(0.01, 0.99, 100)\n",
    "f1_scores_val = [f1_score(y_val, (y_score_val >= thr).astype(int)) for thr in thresholds_f1]\n",
    "thr_f1_optimal = thresholds_f1[np.argmax(f1_scores_val)]\n",
    "\n",
    "\n",
    "# --- Compare all policies on the TEST set, now including F1 score ---\n",
    "rows = []\n",
    "# Add the new F1-optimized approach to the list\n",
    "approaches = [\n",
    "    (\"1. Workload budget\", thr_work),\n",
    "    (\"2. Recall floor\",    thr_rec),\n",
    "    (\"3. Cost min emp\",    thr_cost_emp),\n",
    "    (\"4. F1 Optimized\",    thr_f1_optimal)\n",
    "]\n",
    "\n",
    "for approach, thr in approaches:\n",
    "    # Get the standard summary metrics on the test set\n",
    "    s = summary_at_threshold(y_test, y_score_test, thr).iloc[0].to_dict()\n",
    "    \n",
    "    # Calculate the F1 score on the test set for this threshold\n",
    "    y_pred_test = (y_score_test >= thr).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Update the dictionary with all relevant info\n",
    "    s.update({\n",
    "        \"approach\": approach,\n",
    "        \"expected_cost\": float(C_FP * s[\"FP\"] + C_FN * s[\"FN\"]),\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "    rows.append(s)\n",
    "\n",
    "# Create the final comparison DataFrame, now including the \"F1 Score\" column\n",
    "test_compare = (\n",
    "    pd.DataFrame(rows)[\n",
    "        [\"approach\", \"threshold\", \"precision\", \"recall\", \"F1 Score\",\n",
    "         \"alerts_per_1000\", \"true_pos_per_1000\", \"TP\", \"FP\", \"FN\", \"expected_cost\"]\n",
    "    ]\n",
    "    .sort_values(\"approach\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Pivot the DataFrame for the final transposed view\n",
    "pivot_df = (\n",
    "    test_compare.set_index(\"approach\").T.reset_index().rename(columns={\"index\": \"metric\"})\n",
    ")\n",
    "\n",
    "display(pivot_df.round(3).set_index(\"metric\"))\n",
    "\n",
    "# --- Original threshold-independent metrics ---\n",
    "test_roc_auc = float(roc_auc_score(y_test, y_score_test))\n",
    "test_pr_auc  = float(average_precision_score(y_test, y_score_test))\n",
    "print(f\"Test ROC AUC={test_roc_auc:.3f}  |  Test PR AUC={test_pr_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed882ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Threshold Selection (from your \"Recall Floor\" policy) ---\n",
    "# This ensures THR is set to the value derived from the validation set analysis.\n",
    "THR = thr_rec\n",
    "\n",
    "print(f\"--- Visualizing Final Performance on Test Set at Threshold = {THR:.2f} ---\")\n",
    "\n",
    "# --- Step 1: Get scores and metrics on the TEST set ---\n",
    "y_score_test = positive_scores(calibrated_clf, X_test)\n",
    "\n",
    "# For the first plot, we need the actual recall achieved on the test set by our threshold.\n",
    "y_pred_test = (y_score_test >= THR).astype(int)\n",
    "recall_at_thr_test = recall_score(y_test, y_pred_test)\n",
    "\n",
    "\n",
    "# --- Step 2: Generate the plots sequentially using your utils functions ---\n",
    "\n",
    "# Plot 1: Recall Floor Curves\n",
    "print(\"\\\\n1. Precision-Recall Curve with Final Threshold and Resulting Recall\")\n",
    "plot_recall_floor_curves(y_true=y_test,\n",
    "                         y_score=y_score_test,\n",
    "                         recall_floor=recall_at_thr_test,\n",
    "                         chosen_threshold=THR)\n",
    "\n",
    "# Plot 2: Cumulative Recall vs. Alerts\n",
    "print(\"\\\\n2. Cumulative Recall vs. Number of Alerts\")\n",
    "plot_cumulative_recall_at_threshold(y_true=y_test,\n",
    "                                     y_score=y_score_test,\n",
    "                                     chosen_threshold=THR)\n",
    "\n",
    "# Plot 3: Top-k Patient Prioritization\n",
    "print(\"\\\\n3. Top 30 Highest-Risk Patients View\")\n",
    "plot_topk_at_threshold(y_true=y_test,\n",
    "                       y_score=y_score_test,\n",
    "                       chosen_threshold=THR,\n",
    "                       top_k=30)\n",
    "\n",
    "# Generate the report and plots for the final model on the test set\n",
    "auc_report(y_true=y_test,\n",
    "           y_score=y_score_test,\n",
    "           name=\"Final Calibrated Model on Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581b38e",
   "metadata": {},
   "source": [
    "# 9. Intro to RAI toolkit\n",
    "MS RAI Toolkit helps us inspecting our model beyond global metrics: it supports interpretability, fairness, error slice analysis, counterfactuals and causal probing \n",
    "\n",
    "- Our calibrated classifier makes predictions; RAI lets us explain how features drive those predictions in your data\n",
    "- RAI surfaces groups (e.g. by “Low_inc”, “Surgery”) where model errors (false negatives/positives) may provide potential \"biases\" \n",
    "- It supports what-if and counterfactual analysis so we can test threshold policies more transparently\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Define train and test datasets + metadata (especially identity/sensitive features)\n",
    "2. Create RAIInsights(...) with your model, data, target, task type\n",
    "3. Add modules: explainer, error analysis, counterfactuals, causal analysis etc\n",
    "4. Run .compute() then launch dashboard for visual & tabular inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de40c4",
   "metadata": {},
   "source": [
    "## 9.1 Prepping RAI datasets and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not globals().get(\"_RAI\", False):\n",
    "    raise ImportError(\n",
    "        \"Section 9 requires Responsible AI optional packages (responsibleai, raiwidgets, interpret-community, erroranalysis).\"\n",
    "    )\n",
    "\n",
    "# 1) Resolve target name robustly\n",
    "target_col = (\n",
    "    target_col if \"target_col\" in globals() and isinstance(target_col, str)\n",
    "    else (y_train.name if hasattr(y_train, \"name\") and y_train.name is not None else \"OD\")\n",
    ")\n",
    "\n",
    "# 2) Build train and test DataFrames including target\n",
    "train_df_rai = X_train.copy()\n",
    "train_df_rai[target_col] = pd.Series(y_train).values\n",
    "\n",
    "test_df_rai = X_test.copy()\n",
    "test_df_rai[target_col] = pd.Series(y_test).values\n",
    "\n",
    "# 3) Resolve categorical-like binary features\n",
    "if \"cat_like_binary_cols\" in globals() and isinstance(cat_like_binary_cols, (list, tuple)):\n",
    "    cat_like_binary_cols = list(cat_like_binary_cols)\n",
    "else:\n",
    "    # Infer as columns with only {0,1} after dropping NaN\n",
    "    def is_binary(col: pd.Series) -> bool:\n",
    "        vals = pd.unique(col.dropna())\n",
    "        return set(vals).issubset({0, 1})\n",
    "    cat_like_binary_cols = [c for c in X_train.columns if is_binary(X_train[c])]\n",
    "\n",
    "# 4) Identity feature for fairness slicing\n",
    "# Choose which setting you want by commenting/uncommenting\n",
    "\n",
    "# --- Option A: Single identity feature ---\n",
    "identity_name = \"Low_inc\" if \"Low_inc\" in X_train.columns else None\n",
    "\n",
    "# --- Option B: Multiple identity features ---\n",
    "# identity_name = None   # RAIInsights only takes a single \"identity_feature_name\"\n",
    "# identity_features = [\"Low_inc\", \"Surgery\"]\n",
    "# You can still slice by multiple features manually in fairness analysis (Fairlearn MetricFrame)\n",
    "\n",
    "# --- Option C: No identity feature defined ---\n",
    "# identity_name = None\n",
    "\n",
    "feature_metadata = FeatureMetadata(\n",
    "    identity_feature_name=identity_name,\n",
    "    categorical_features=cat_like_binary_cols,\n",
    "    dropped_features=[]\n",
    ")\n",
    "\n",
    "# 5) Classes for interpret dashboards\n",
    "classes = sorted(pd.Series(y_train).dropna().unique().tolist())\n",
    "\n",
    "# 6) Optional threshold wiring if available\n",
    "# Adds calibrated score and decision using your selected threshold\n",
    "if (\n",
    "    \"calibrated_clf\" in globals()\n",
    "    and callable(globals().get(\"positive_scores\", None))\n",
    "    and \"thr_rec\" in globals()\n",
    "):\n",
    "    THR = float(thr_rec)\n",
    "    y_score_test = positive_scores(calibrated_clf, X_test)\n",
    "    test_df_rai[\"score\"] = y_score_test\n",
    "    test_df_rai[\"pred_with_THR\"] = (y_score_test >= THR).astype(int)\n",
    "\n",
    "# Sanity echo\n",
    "print(f\"target_col: {target_col}\")\n",
    "print(f\"identity_feature_name: {feature_metadata.identity_feature_name}\")\n",
    "print(f\"categorical_features: {len(cat_like_binary_cols)} columns\")\n",
    "print(f\"train_df_rai shape: {train_df_rai.shape}, test_df_rai shape: {test_df_rai.shape}\")\n",
    "if \"THR\" in globals():\n",
    "    print(f\"THR set to {THR:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30868869",
   "metadata": {},
   "source": [
    "## 9.2 Build RAIInsights, add components, compute, and render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not globals().get(\"_RAI\", False):\n",
    "    raise RuntimeError(\"Responsible AI extras missing; install optional packages before running this cell.\")\n",
    "\n",
    "# Ensure we pass identical feature columns (plus target) to RAIInsights\n",
    "extra_cols = {\"score\", \"pred_with_THR\", \"pred_before_cal\", \"raw_prob\"}\n",
    "required_cols = [c for c in train_df_rai.columns if c not in extra_cols]\n",
    "if target_col not in required_cols:\n",
    "    required_cols.append(target_col)\n",
    "\n",
    "train_for_rai = train_df_rai[required_cols].copy()\n",
    "test_for_rai = test_df_rai[required_cols].copy()\n",
    "\n",
    "# Create RAIInsights using positional args to be version-safe\n",
    "rai_insights = RAIInsights(\n",
    "    calibrated_clf,\n",
    "    train_for_rai,\n",
    "    test_for_rai,\n",
    "    target_col,\n",
    "    task_type=\"classification\",\n",
    "    categorical_features=cat_like_binary_cols,\n",
    "    feature_metadata=feature_metadata,\n",
    ")\n",
    "\n",
    "# Add components (skip optional ones if dependencies are missing)\n",
    "rai_insights.explainer.add()\n",
    "rai_insights.error_analysis.add()\n",
    "if globals().get(\"_DICE\", False):\n",
    "    rai_insights.counterfactual.add(total_CFs=10, desired_class=\"opposite\")\n",
    "else:\n",
    "    print(\"Skipping counterfactual component; install dice-ml to enable it.\")\n",
    "\n",
    "if globals().get(\"_ECONML\", False):\n",
    "    rai_insights.causal.add(treatment_features=[\"rx_ds\", \"Surgery\"])\n",
    "else:\n",
    "    print(\"Skipping causal component; install econml to enable it.\")\n",
    "\n",
    "# Compute and persist\n",
    "rai_insights.compute()\n",
    "rai_insights.save(\"rai_od_dashboard\")\n",
    "\n",
    "# Launch dashboard\n",
    "ResponsibleAIDashboard(rai_insights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444c390",
   "metadata": {},
   "source": [
    "## 9.3 Standalone SHAP dashboard with TabularExplainer\n",
    "Separate from RAIInsights, but is useful if we want to focuse on interpretability or exporting a global explanation artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standalone SHAP global explanation on the calibrated pipeline for the test set\n",
    "explainer = TabularExplainer(\n",
    "    model=calibrated_clf,\n",
    "    initialization_examples=X_train,\n",
    "    features=X_train.columns,\n",
    "    classes=classes\n",
    ")\n",
    "\n",
    "global_explanation = explainer.explain_global(X_test)\n",
    "\n",
    "# Launch the Explanation dashboard for interpret-community Explanation objects\n",
    "ExplanationDashboard(\n",
    "    global_explanation,\n",
    "    calibrated_clf,\n",
    "    dataset=X_test,\n",
    "    true_y=y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b6f64",
   "metadata": {},
   "source": [
    "## 9.4. Fairness diagnostics aligned with chosen threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6cc3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure THR is already set by your threshold selection section\n",
    "assert \"THR\" in globals(), \"THR must be set earlier from your chosen policy\"\n",
    "assert isinstance(THR, float), \"THR should be a float threshold between 0 and 1\"\n",
    "\n",
    "# Scores and hard decisions on test\n",
    "y_score_test = positive_scores(calibrated_clf, X_test)\n",
    "y_pred_test = (y_score_test >= THR).astype(int)\n",
    "\n",
    "# Choose sensitive and operational slices\n",
    "slice_cols = [\"Low_inc\", \"Surgery\"]\n",
    "\n",
    "# MetricFrame gives per-group metrics and overall metrics\n",
    "mf = MetricFrame(\n",
    "    metrics={\n",
    "        \"selection_rate\": selection_rate,\n",
    "        \"tpr\": true_positive_rate,\n",
    "        \"fpr\": false_positive_rate,\n",
    "        \"fnr\": false_negative_rate,\n",
    "    },\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_test,\n",
    "    sensitive_features=X_test[slice_cols]\n",
    ")\n",
    "\n",
    "display(mf.by_group)\n",
    "print(\"Overall:\", mf.overall)\n",
    "print(\"Between-group disparity (max - min) by metric:\")\n",
    "for m in [\"selection_rate\", \"tpr\", \"fpr\", \"fnr\"]:\n",
    "    series = mf.by_group[m]\n",
    "    print(f\"  {m}: {series.max() - series.min():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade5b750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ce4a28b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebed8a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "od_rai_lgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
